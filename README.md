# Paper-List-for-DL-and-CV-Beginners
In this repository, we share a curated collection of papers to help beginners get started with deep learning and computer vision
## Visual Place Recognition
BoQ: A Place is Worth a Bag of Learnable Queries. CVPR24 

Deep Homography Estimation for Visual Place Recognition. AAAI24 

CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition. CVPR24 

EDTformer: An Efficient Decoder Transformer for Visual Place Recognition. TCSVT25

TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place Recognition. RAL25

SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition. Neurips24

Optimal Transport Aggregation for Visual Place Recognition. CVPR24

Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition. ICLR24

MixVPR: Feature Mixing for Visual Place Recognition. WACV23

Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition. AAAI25

EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition. ICCV23

Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era. Neurips25

MeshVPR: Citywide Visual Place Recognition Using 3D Meshes. ECCV24

Rethinking Visual Geo-localization for Large-Scale Applications. CVPR22

AnyLoc: Towards Universal Visual Place Recognition. ICRA24
 
## General Visual Tasks

OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels. CVPR25

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR21

LeViT: a Vision Transformer in ConvNetâ€™s Clothing for Faster Inference. ICCV21

Is Space-Time Attention All You Need for Video Understanding? ICML21

Masked Autoencoders Are Scalable Vision Learners. CVPR22

Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV21.

Not all patches are what you need: Expediting vision transformers via token reorganization. ICLR22

Vision Transformer with Deformable Attention. CVPR22

Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model.ICML24

MambaOut: Do We Really Need Mamba for Vision? CVPR25

Vision Transformers Need Registers. ICLR24

Segment Anything. ICCV23

## Local Feature Macth

SFD2: Semantic-guided Feature Detection and Description. CVPR23

DKM: Dense Kernelized Feature Matching for Geometry Estimation. CVPR23

RoMa: Robust Dense Feature Matching. CVPR24.

Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed. CVPR24

EDM: Efficient Deep Feature Matching. ICCV25

CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching. ICCV25

JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba. CVPR25

LoFTR: Detector-Free Local Feature Matching with Transformers. CVPR21

## Visual Foundation Models

VGGT: Visual Geometry Grounded Transformer. CVPR25

DINOv2: Learning Robust Visual Features without Supervision. TMLR24

DINOv3. Arxiv25

## Diffusion 
Do Text-free Diffusion Models Learn Discriminative Visual Representations? ECCV24

Diffusion Models and Representation Learning: A Survey. TPAMI23

## Knowledge Distillation

Improving Language Model Distillation through Hidden State Matching. ICLR25

Rethinking Centered Kernel Alignment in Knowledge Distillation. IJCAI24

Single teacher, multiple perspectives: Teacher knowledge augmentation for enhanced knowledge distillation. ICLR25


## LLM

Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free. Neurips25

## Others

Attention Is All You Need. Neurips17

Mamba: Linear-Time Sequence Modeling with Selective State Spaces. COLM24.


